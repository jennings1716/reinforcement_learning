{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Reinforcement learning algorithms try to find the best ways to earn the greatest reward. Rewards can be winning a game, earning more money or beating other opponents.\n",
    "\n",
    "The reinforcement learning process can be modeled as an iterative loop that works as below:\n",
    "\n",
    "    1. The RL Agent receives state S‚Å∞ from the environment.\n",
    "    \n",
    "    2. Based on that state S‚Å∞, the RL agent takes an action A‚Å∞, say‚Ää‚Äî‚Ääour RL agent moves right. Initially, this is random.\n",
    "    \n",
    "    3. Now, the environment is in a new state S¬π\n",
    "    \n",
    "    4. Environment gives some reward R¬π to the RL agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RL_1.png\" width=\"500\" style=\"left:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Maximization\n",
    "\n",
    "**Reinforcement learning should have best possible action in order to maximize the reward**\n",
    "\n",
    "<img src=\"RL_2.png\" width=\"300\" style=\"left:1px\">\n",
    "\n",
    "But this way of adding the rewards doesnt work well because of the uncertainity factor.\n",
    "\n",
    "<img src=\"RL_3.png\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "# Tasks and their types\n",
    "\n",
    "A task is a single instance of a reinforcement learning problem.\n",
    "\n",
    "## Types\n",
    "    - Continuous tasks\n",
    "    - Episodic tasks\n",
    "    \n",
    "### Continuous Tasks\n",
    "    These are the tasks that continue forever and the agent continuous to act until the process is stopped Manually\n",
    "    Eg: RL Agent which does the automated trading\n",
    "    \n",
    "### Episodic Tasks\n",
    "    These tasks have starting and Ending point. \n",
    "    Eg: Game Either we win or Opponent wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and exploitation trade off\n",
    "\n",
    "    Exploration is the process of finding more information about the Environment.\n",
    "    \n",
    "    Exploitation is the process of exploiting the known information to maximize the rewards.\n",
    "    \n",
    "    Real Life Example: \n",
    "        Say you go to the same restaurant every day. You are basically exploiting. But on the other hand, if you search for new restaurant every time before going to any one of them, then it‚Äôs exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to Reinforcement Learning\n",
    "\n",
    "There are three approaches to solve the Reinforcement learning problem:\n",
    "   1. **Policy based approach**\n",
    "            We learn a policy function which helps us in mapping each state to the best action.\n",
    "            We have a policy which we need to optimize\n",
    "            We further divide policies into two types:\n",
    "            \n",
    "              Deterministic \n",
    "                  A policy at a given state(s) will always return the same action(a). \n",
    "                  It means, it is pre-mapped as **S=(s) ‚û° A=(a)**.\n",
    "              Stochastic \n",
    "                  It gives a distribution of probability over different actions. \n",
    "                  i.e Stochastic Policy ‚û° **p( A = a | S = s )**\n",
    "            \n",
    "   2. **Value Based approach**\n",
    "            We have to optimize the function which tells the maximum expected future reward at each state.\n",
    "        \n",
    "            The value of each state is the total amount of the reward an RL agent can expect to collect over the future, \n",
    "            from a particular state.\n",
    "            \n",
    "            \n",
    "<img src=\"RL_4.png\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "             The agent will always take the state with the biggest value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process\n",
    "\n",
    "The process of the agent observing the environment output consisting of a reward and the next state, and then acting upon that. This whole process is a **Markov Decision Process**\n",
    "\n",
    "To understand the MDP, first we have to understand the Markov property.\n",
    "\n",
    "**The Markov property**\n",
    "    _‚Äú The future is independent of the past given the present.‚Äù_\n",
    "    \n",
    "    P[St+1 | St] = P[St+1 | S1, ‚Ä¶.. , St],\n",
    "    \n",
    "    For a Markov state S and successor state S‚Ä≤, the state transition probability function is defined by,\n",
    "    \n",
    "<img src=\"RL_5.png\" width=\"200\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "**Markov Process**\n",
    "    A Markov process is a memory-less random process, i.e. a sequence of random states S1, S2, ‚Ä¶.. **with the Markov property**. A Markov process or Markov chain is a tuple (S, P) on state space S, and transition function P. The dynamics of the system can be defined by these two components S and P. \n",
    "    \n",
    "    \n",
    "**Markov Reward Process**\n",
    "    A Markov Reward Process or an MRP is a Markov process with value judgment, saying how much reward accumulated through some particular sequence that we sampled.\n",
    "    \n",
    "    An MRP is a tuple (S, P, R, ùõæ) where S is a finite state space, P is the state transition probability function, R is a reward function where\n",
    "    \n",
    "<img src=\"RL_6.png\" width=\"200\" style=\"left:1px\">    \n",
    "    \n",
    "**Bellman Equation**\n",
    "    The agent tries to get the most expected sum of rewards from every state it lands in.\n",
    "    \n",
    "    We unroll the return Gt,\n",
    "<img src=\"RL_7.png\" width=\"200\" style=\"left:1px\">    \n",
    "\n",
    "That gives us the Bellman equation for MRPs,\n",
    "\n",
    "<img src=\"RL_8.png\" width=\"300\" style=\"left:1px\">    \n",
    "    \n",
    "    \n",
    "**The value of the state S is the reward we get upon leaving that state, plus a discounted average over next possible successor states, where the value of each possible successor state is multiplied by the probability that we land in it.**\n",
    "    \n",
    "<img src=\"RL_9.png\" width=\"300\" style=\"left:1px\">    \n",
    "    \n",
    "    \n",
    "**Markov Decision process**\n",
    "An MDP is a Markov Reward Process with decisions, it‚Äôs an environment in which all states are Markov. This is what we want to solve.\n",
    "\n",
    "An MDP is a tuple (S, A, P, R, ùõæ), where S is our state space, A is a finite set of actions, P is the state transition probability function,\n",
    "\n",
    "<img src=\"RL_10.png\" width=\"300\" style=\"left:1px\">    \n",
    "\n",
    "R is the reward function\n",
    "\n",
    "<img src=\"R_11.png\" width=\"300\" style=\"left:1px\"> \n",
    "\n",
    "and ùõæ is a discount factor ùõæ ‚àà [0, 1].\n",
    "\n",
    "\n",
    "Remember that a policy ùúã is a distribution over actions given states. A policy fully defines the behavior of an agent,\n",
    "\n",
    "<img src=\"RL_12.png\" width=\"300\" style=\"left:1px\">\n",
    "\n",
    "The state-value function Vùúã(s) of an MDP is the expected return starting from state S, and then following policy ùúã.\n",
    "\n",
    "### The value function tells us how good is it to be in state S if I‚Äôm following policy ùúã, i.e. the expectations when we sample all actions according to policy ùúã\n",
    "\n",
    "<img src=\"RL_17.png\" width=\"700\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "### \"The action-value function qùúã(s, a) is the expected return starting from state s, taking action a, and following policy ùúã. ‚Äú\n",
    "\n",
    "<img src=\"RL_13.png\" width=\"500\" style=\"left:1px\">    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bellman Expectation Equation\n",
    "\n",
    "<img src=\"R_18.png\" width=\"500\" style=\"left:1px\">    \n",
    "\n",
    "\n",
    "From a particular state S, there are multiple actions, I‚Äôm gonna average over the actions that I might take. \n",
    "\n",
    "### Vùúã(s) is telling us how good is it to be in a particular state,\n",
    "\n",
    "<img src=\"RL_16.png\" width=\"300\" style=\"left:1px\">\n",
    "\n",
    "### qùúã(s, a) is telling us how good is it to take a particular action from a given state.\n",
    "\n",
    "<img src=\"RL_14.png\" width=\"300\" style=\"left:1px\">\n",
    "            \n",
    "***Stitching Bellman expectation equation for V***\n",
    "\n",
    "<img src=\"RL_15.png\" width=\"400\" style=\"left:1px\">    \n",
    "\n",
    "***Stitching Bellman expectation equation for q***\n",
    "<img src=\"RL_19.png\" width=\"400\" style=\"left:1px\">    \n",
    "\n",
    "  ***Finally  we get the immediate reward for our action, and then we average over possible states we might land in, i.e. the value of each state we might land in multiplied by a probability the environment will select and average over all those things together***.\n",
    "\n",
    "<img src=\"R_20.png\" width=\"600\" style=\"left:1px\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Value Functions\n",
    "\n",
    "**The optimal state-value function V*(s) is the maximum value function over all policies.**\n",
    "\n",
    "<img src=\"RL_21.png\" width=\"300\" style=\"left:1px\">    \n",
    "\n",
    "**The optimal action-value function q*(s, a) is the maximum action value function over all policies.**\n",
    "\n",
    "<img src=\"RL_22.png\" width=\"300\" style=\"left:1px\">    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
