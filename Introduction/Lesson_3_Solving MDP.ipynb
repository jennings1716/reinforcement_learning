{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "## Dynamic Programming for Model based Reinforcement Learning\n",
    "  Dynamic Programming or (DP) is a **method for solving complex problems** by breaking them down into subproblems, solve the subproblems, and combine solutions to the subproblems to solve the overall problem.\n",
    "  \n",
    "  It is the general solution for the problems which has the following two properties:\n",
    "      \n",
    "      1. \"Optimal Structure\" where the principle of Optimality applies.\n",
    "                It tells us that we can solve some overall problem by breaking it down into two or more pieces, solve for            each of the pieces, and the optimal solution to the pieces tells us how to get the optimal solution to the overall          problem.\n",
    "      \n",
    "      2. ‚Äúoverlapping subproblems‚Äù, which means that the subproblems which occur once, will occur again and again.\n",
    "                 We broke down the problem of getting to the wall into first getting to the midpoint, and then getting to \n",
    "         the wall. That will help us solve the subproblem of how to get from another point to the wall. Solutions can be              cached and reused.\n",
    "    \n",
    "    **Markov Decision Processes satisfy both mentioned properties**\n",
    "\n",
    "There are two main ideas we tackle in a given MDP. If someone tells us the MDP, where M = (S, A, P, R, ùõæ), and a policy ùúã or an MRP where M = (S, P, R, ùõæ), we can do prediction, i.e. evaluate the given policy to get the value function on that policy.\n",
    "\n",
    "The other main idea is, we are given an MDP, M = (S, A, P, R, ùõæ) and we are asked for the optimal value function and optimal policy.\n",
    "\n",
    "**In policy iteration algorithms, you start with a random policy, then find the value function of that policy (policy evaluation step), then find a new (improved) policy based on the previous value function, and so on. In this process, each policy is guaranteed to be a strict improvement over the previous one (unless it is already optimal). Given a policy, its value function can be obtained using the Bellman operator.**\n",
    "\n",
    "**In value iteration, you start with a random value function and then find a new (improved) value function in an iterative process, until reaching the optimal value function. Notice that you can derive easily the optimal policy from the optimal value function. This process is based on the optimality Bellman operator.**\n",
    "\n",
    "**A efficient method is to incrementally find the value function for a specific policy and then use the policy which maximizes this value function for the next round. Called ‚ÄúPolicy iteration‚Äù**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy iteration =  Policy evaluation + Policy improvement \n",
    "This algorithm has 3 steps\n",
    "\n",
    "    1. Policy evaluation (based on policy œÄ (initially random) ,calculate value function)\n",
    "    2. Policy improvement (improve the policy based on the value function)\n",
    "    3. Repeat 1 and 2 until œÄ converges\n",
    "\n",
    "<img src=\"images/RL_32.jpeg\" width=\"500\" style=\"left:1px\"> \n",
    "\n",
    "\n",
    "### Policy Evaluation\n",
    "\n",
    "In policy evaluation, we‚Äôre given an MDP and a policy ùúã. We try to evaluate the given policy. We can evaluate the given policy through iterative application of Bellman expectation equation, and later use the Bellman optimality equation to do control.\n",
    "\n",
    " \n",
    "\n",
    "### Policy Improvement\n",
    "Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function Vùúã for an arbitrary deterministic policy ùúã. For some state s we would like to know whether or not we should change the policy to deterministically choose an action a ‚â† ùúã(s).\n",
    "\n",
    "It would be better to consider the action at the specific state.\n",
    "\n",
    "So formally, given a policy ùúã, we‚Äôre going to evaluate it, i.e. we‚Äôre going to figure out a value function for that policy and then we improve the policy by acting greedily with respect to Vùúã\n",
    "\n",
    "<img src=\"images/RL_31.png\" width=\"400\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value iteration = optimal value function + policy extraction\n",
    "\n",
    "This algorithm has two steps\n",
    "\n",
    "    1. it focuses on finding the optimal state value function, once the optimal state value function is found then\n",
    "    \n",
    "    2. it extracts the optimal policy from it.\n",
    "    \n",
    "<img src=\"images/RL_33.jpeg\" width=\"500\" style=\"left:1px\"> \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Free Reinforcement Learning\n",
    "\n",
    "    let me define couple of concepts so the rest is gonna be pretty easy to understand.\n",
    "\n",
    "         1. Model Free Prediction - Estimate the value function of unknown MDP\n",
    "    \n",
    "         2. Model Free Control - Optimize the value function of unknown MDP \n",
    "\n",
    "\n",
    "There are two types of policy learning methods\n",
    "\n",
    "        1. On policy learning : It learns on the job. which means it evaluates or improves the policy that is used to make              the decisions.\n",
    "           (In other words) it directly learns a policy which gives you decisions about which action to take in some state.\n",
    "\n",
    "        2. Off policy learning : It evaluates one policy ( target policy ) while following another policy ( behavior policy)\n",
    "           just like we learn to do something while observing others doing the same thing.\n",
    "\n",
    "In RL problems we have two different tasks in nature.\n",
    "\n",
    "        1. Episodic task : A task which can last a finite amount of time is called Episodic task ( an episode )\n",
    "        \n",
    "        2. Continuous task : A task which never ends is called Continuous task.\n",
    "        \n",
    "**In MDP , we are given all the components to solve a problem, but what if we are not given some of the components ???**\n",
    "\n",
    "In Model-free , we just focus on figuring out the value functions directly from the interactions with the environment\n",
    "\n",
    "All model free learning algorithms are gonna the learn value functions directly from the environment.\n",
    "\n",
    "There are few approaches for solving these kind of problems\n",
    "\n",
    "    1. Monte carlo approach\n",
    "\n",
    "    2. Temporal-Difference approach.\n",
    "    \n",
    "\n",
    "## Monte carlo approach\n",
    "\n",
    "- it learns value functions directly from episodes of experience.\n",
    "    \n",
    "- We only get the reward at the end of an episode\n",
    " \n",
    "### Prediction Tasks\n",
    "\n",
    "v(s) = E [Gt | St = s] and Gt = Rt+1+ Œ≥Rt+2+‚Ä¶\n",
    "    \n",
    "we learn value functions from sample returns with the MDP\n",
    "\n",
    "sample return could mean average of returns(rewards) from episodes\n",
    "    \n",
    "There are two different types in MC\n",
    "\n",
    "    1. First visit MC\n",
    "    in this, we average returns only for first time s is visited in an episode\n",
    "\n",
    "    2. Every visit MC\n",
    "\n",
    "    in this, we average returns for every time s is visited in an episode\n",
    "    \n",
    "    \n",
    "V(S) is the average of G\n",
    "\n",
    "we know how to calculate the mean if we have all the values but what if we don‚Äôt have all the values ( we get values as episodes pass) ??? how to calculate the mean ???\n",
    "\n",
    "there is an approach called **Incremental mean**\n",
    "\n",
    "<img src=\"images/RL_34.jpeg\" width=\"600\" style=\"left:1px\">\n",
    "\n",
    "### Control Tasks\n",
    "\n",
    "Policy iteration( we discussed in the last story ) is used for the control tasks\n",
    "\n",
    "it has policy evaluation and policy improvement.\n",
    "\n",
    "<img src=\"images/RL_36.jpeg\" width=\"500\" style=\"left:1px\"> \n",
    "\n",
    "<img src=\"images/RL_37.png\" width=\"500\" style=\"left:1px\"> \n",
    "\n",
    "\n",
    "## Exploration vs Exploitation\n",
    "\n",
    "**Exploration** : is about finding more information about the environment. (in other words) exploring a lot of states and actions in the environment.\n",
    "\n",
    "**Exploitation** : is about exploiting the known information to maximize the reward.\n",
    "\n",
    "\n",
    "<center> <strong> DP vs MC  </strong>  </center>\n",
    "\n",
    "<img src=\"images/RL_38.png\" width=\"700\" style=\"left:1px\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference = DP + MC\n",
    "\n",
    "Temporal difference is the combination of Monte Carlo and Dynamic Programming.\n",
    "\n",
    "We use epsilon greedy policy to avoid the exploration problem in the env.\n",
    "\n",
    "**TD Prediction**\n",
    "<img src=\"images/RL_39.jpeg\" width=\"500\" style=\"left:1px\"> \n",
    "\n",
    "**TD Control**\n",
    "\n",
    "Just like in Monte Carlo , we use policy iteration for TD Control\n",
    "\n",
    "There are two algorithms in TD control\n",
    "\n",
    "    1. SARSA ( state-action-reward-state-action)\n",
    "    \n",
    "    On policy learning method , means it uses the same policy to choose the next action A`\n",
    "\n",
    "<img src=\"images/RL_40.jpeg\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "    2. Q-learning\n",
    "\n",
    "        One of the most important breakthroughs in reinforcement learning was the development of an off-policy TD control algorithm known as Q-learning\n",
    "\n",
    "        Q-learning estimates a state-action value function for a target policy that deterministically selects the action of highest value\n",
    "\n",
    "<img src=\"images/RL_41.jpeg\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "<img src=\"images/RL_42.jpeg\" width=\"1400\" style=\"left:1px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
