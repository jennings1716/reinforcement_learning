{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Reinforcement learning algorithms try to find the best ways to earn the greatest reward. Rewards can be winning a game, earning more money or beating other opponents.\n",
    "\n",
    "The reinforcement learning process can be modeled as an iterative loop that works as below:\n",
    "\n",
    "    1. The RL Agent receives state S⁰ from the environment.\n",
    "    \n",
    "    2. Based on that state S⁰, the RL agent takes an action A⁰, say — our RL agent moves right. Initially, this is random.\n",
    "    \n",
    "    3. Now, the environment is in a new state S¹\n",
    "    \n",
    "    4. Environment gives some reward R¹ to the RL agent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "Agent has one or more of the following components:\n",
    "    1. Policy:Behavior of the agent\n",
    "    2. Value function: How good a state is\n",
    "    3. Model: Agent's representation of the environment.\n",
    "\n",
    "### Policy\n",
    "It is a map from state to action.\n",
    "\n",
    "Deterministic Policy    **a = π(s)**\n",
    "\n",
    "Stochastic Policy **π(a/s) = P(A=a/S=s)**\n",
    "\n",
    "\n",
    "### Value function\n",
    "\n",
    "Value function is the prediction of the future reward.\n",
    "\n",
    "Value function is to select between states.\n",
    "\n",
    "<img src=\"RL_4.png\" width=\"400\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "### Model\n",
    "\n",
    "A model predicts what environment will do next\n",
    "\n",
    "**Transition:   P** Predicts the next state\n",
    "\n",
    "**Rewards:      R** Predicts the next(immediate) reward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"RL_1.png\" width=\"500\" style=\"left:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Maximization\n",
    "\n",
    "**Reinforcement learning should have best possible action in order to maximize the reward**\n",
    "\n",
    "<img src=\"RL_2.png\" width=\"300\" style=\"left:1px\">\n",
    "\n",
    "But this way of adding the rewards doesnt work well because of the uncertainity factor.\n",
    "\n",
    "<img src=\"RL_3.png\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "# Tasks and their types\n",
    "\n",
    "A task is a single instance of a reinforcement learning problem.\n",
    "\n",
    "## Types\n",
    "    - Continuous tasks\n",
    "    - Episodic tasks\n",
    "    \n",
    "### Continuous Tasks\n",
    "    These are the tasks that continue forever and the agent continuous to act until the process is stopped Manually\n",
    "    Eg: RL Agent which does the automated trading\n",
    "    \n",
    "### Episodic Tasks\n",
    "    These tasks have starting and Ending point. \n",
    "    Eg: Game Either we win or Opponent wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration and exploitation trade off\n",
    "\n",
    "    Exploration is the process of finding more information about the Environment.\n",
    "    \n",
    "    Exploitation is the process of exploiting the known information to maximize the rewards.\n",
    "    \n",
    "    Real Life Example: \n",
    "        Say you go to the same restaurant every day. You are basically exploiting. But on the other hand, if you search for new restaurant every time before going to any one of them, then it’s exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approaches to Reinforcement Learning\n",
    "\n",
    "There are three approaches to solve the Reinforcement learning problem:\n",
    "   1. **Policy based approach**\n",
    "            We learn a policy function which helps us in mapping each state to the best action.\n",
    "            We have a policy which we need to optimize\n",
    "            We further divide policies into two types:\n",
    "            \n",
    "              Deterministic \n",
    "                  A policy at a given state(s) will always return the same action(a). \n",
    "                  It means, it is pre-mapped as **S=(s) ➡ A=(a)**.\n",
    "              Stochastic \n",
    "                  It gives a distribution of probability over different actions. \n",
    "                  i.e Stochastic Policy ➡ **p( A = a | S = s )**\n",
    "            \n",
    "   2. **Value Based approach**\n",
    "            We have to optimize the function which tells the maximum expected future reward at each state.\n",
    "        \n",
    "            The value of each state is the total amount of the reward an RL agent can expect to collect over the future, \n",
    "            from a particular state.\n",
    "            \n",
    "            The agent will always take the state with the biggest value\n",
    "            \n",
    "            \n",
    "<img src=\"RL_4.png\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "\n",
    "   3.**Actor Critic**\n",
    "   \n",
    "   \n",
    "          Combines both Policy and Value based approaches and gives the best. \n",
    "\n",
    "<img src=\"RL_23.png\" width=\"300\" style=\"left:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with in Reinforcement Learning\n",
    "\n",
    "    Two main problems in sequential Decision making:\n",
    "    \n",
    "    Reinforcement Learning\n",
    "        The environment is initially unknown\n",
    "        \n",
    "        The Agent interacts with the environment\n",
    "        \n",
    "        The Policy is improved\n",
    "    \n",
    "    Planning\n",
    "         A model of the environment is known.\n",
    "         \n",
    "         The Agent performs computations with the model\n",
    "         \n",
    "         The agent improves the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
