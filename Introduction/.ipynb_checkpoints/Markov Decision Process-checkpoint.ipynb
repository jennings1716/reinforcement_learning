{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Markov Decision Process - describe an environment for RL\n",
    "\n",
    "The process of the agent observing the environment output consisting of a reward and the next state, and then acting upon that. This whole process is a **Markov Decision Process**\n",
    "\n",
    "All RL problems can be formalized as MDP\n",
    "\n",
    "To understand the MDP, first we have to understand the Markov property.\n",
    "\n",
    "**The Markov property**\n",
    "    _‚Äú The future is independent of the past given the present.‚Äù_\n",
    "    \n",
    "    P[St+1 | St] = P[St+1 | S1, ‚Ä¶.. , St],\n",
    "    \n",
    "    For a Markov state S and successor state S‚Ä≤, the state transition probability function is defined by,\n",
    "    \n",
    "<img src=\"RL_5.png\" width=\"200\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "**Markov Process**\n",
    "    A Markov process is a memory-less random process, i.e. a sequence of random states S1, S2, ‚Ä¶.. **with the Markov property**. A Markov process or Markov chain is a tuple (S, P) on state space S, and transition function P. The dynamics of the system can be defined by these two components S and P. \n",
    "    \n",
    "    **Pss' = P[St+1=s'|St=s]**\n",
    "    \n",
    "    \n",
    "**Markov Reward Process**\n",
    "    A Markov Reward Process or an MRP is a Markov process with value judgment, saying how much reward accumulated through some particular sequence that we sampled.\n",
    "    \n",
    "    An MRP is a tuple (S, P, R, ùõæ) where S is a finite state space, P is the state transition probability function, R is a reward function where ùõæ is the discount factor\n",
    "    \n",
    "    Rs = ùîº[Rt+1 | St = S],\n",
    "    \n",
    "**Return**\n",
    "The total discounted discounted reward from timestep t\n",
    "\n",
    "<img src=\"RL_24.png\" width=\"500\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "**Why Discount ?**\n",
    "Uncertainity about the future may not be fully represented\n",
    "\n",
    "\n",
    "**Value Function**\n",
    "\n",
    "Expected Return starting from state S\n",
    "\n",
    "\n",
    " <img src=\"RL_6.png\" width=\"200\" style=\"left:1px\">    \n",
    "    \n",
    "**Bellman Equation**\n",
    "    The agent tries to get the most expected sum of rewards from every state it lands in.\n",
    "    \n",
    "    We unroll the return Gt,\n",
    "<img src=\"RL_7.png\" width=\"300\" style=\"left:1px\">    \n",
    "\n",
    "That gives us the Bellman equation for MRPs,\n",
    "\n",
    "<img src=\"RL_8.png\" width=\"300\" style=\"left:1px\">    \n",
    "    \n",
    "    \n",
    "**The value of the state S is the reward we get upon leaving that state, plus a discounted average over next possible successor states, where the value of each possible successor state is multiplied by the probability that we land in it.**\n",
    "    \n",
    "<img src=\"RL_9.png\" width=\"300\" style=\"left:1px\">    \n",
    "    \n",
    "    \n",
    "## Markov Decision process\n",
    "An MDP is a Markov Reward Process with decisions, it‚Äôs an environment in which all states are Markov. This is what we want to solve.\n",
    "\n",
    "An MDP is a tuple (S, A, P, R, ùõæ), where S is our state space, A is a finite set of actions, P is the state transition probability function,\n",
    "\n",
    "<img src=\"RL_10.png\" width=\"300\" style=\"left:1px\">    \n",
    "\n",
    "R is the reward function\n",
    "\n",
    "<img src=\"R_11.png\" width=\"300\" style=\"left:1px\"> \n",
    "\n",
    "and ùõæ is a discount factor ùõæ ‚àà [0, 1].\n",
    "\n",
    "\n",
    "Remember that a policy ùúã is a distribution over actions given states. A policy fully defines the behavior of an agent,\n",
    "\n",
    "<img src=\"RL_12.png\" width=\"300\" style=\"left:1px\">\n",
    "\n",
    "The state-value function Vùúã(s) of an MDP is the expected return starting from state S, and then following policy ùúã.\n",
    "\n",
    "### The state-value function tells us how good is it to be in state S if I‚Äôm following policy ùúã, i.e. the expectations when we sample all actions according to policy ùúã\n",
    "\n",
    "<img src=\"RL_17.png\" width=\"700\" style=\"left:1px\">\n",
    "\n",
    "\n",
    "### \"The action-value function qùúã(s, a) is the expected return starting from state s, taking action a, and following policy ùúã. ‚Äú(How good is to take a particular action a at the state s)\n",
    "\n",
    "<img src=\"RL_13.png\" width=\"500\" style=\"left:1px\">    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Expectation Equation\n",
    "\n",
    "<img src=\"R_18.png\" width=\"500\" style=\"left:1px\">    \n",
    "\n",
    "\n",
    "From a particular state S, there are multiple actions, I‚Äôm gonna average over the actions that I might take. \n",
    "\n",
    "### Vùúã(s) is telling us how good is it to be in a particular state,\n",
    "\n",
    "<img src=\"RL_16.png\" width=\"300\" style=\"left:1px\">\n",
    "\n",
    "### qùúã(s, a) is telling us how good is it to take a particular action from a given state.\n",
    "\n",
    "<img src=\"RL_14.png\" width=\"300\" style=\"left:1px\">\n",
    "            \n",
    "***Stitching Bellman expectation equation for V***\n",
    "\n",
    "<img src=\"RL_15.png\" width=\"400\" style=\"left:1px\">    \n",
    "\n",
    "***Stitching Bellman expectation equation for q***\n",
    "<img src=\"RL_19.png\" width=\"400\" style=\"left:1px\">    \n",
    "\n",
    "  ***Finally  we get the immediate reward for our action, and then we average over possible states we might land in, i.e. the value of each state we might land in multiplied by a probability the environment will select and average over all those things together***.\n",
    "\n",
    "<img src=\"R_20.png\" width=\"600\" style=\"left:1px\">    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the best solution for the MDP ?\n",
    "\n",
    "### Optimal Value Functions\n",
    "\n",
    "**The optimal state-value function V*(s) is the maximum value function over all policies.**\n",
    "\n",
    "<img src=\"RL_21.png\" width=\"300\" style=\"left:1px\">    \n",
    "\n",
    "**The optimal action-value function q*(s, a) is the maximum action value function over all policies.**\n",
    "\n",
    "<img src=\"RL_22.png\" width=\"300\" style=\"left:1px\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal  Policy\n",
    "\n",
    "One policy is better than another policy if the value function for that policy is greater than the value function of the other policy in all states.\n",
    "\n",
    "\n",
    "For any Markov Decision Process, there exists an optimal policy ùúã* that is better than or equal to all other policies, ùúã* > ùúã, ‚àÄùúã.\n",
    "\n",
    "\n",
    "It‚Äôs possible to have more than one optimal policy,\n",
    "\n",
    "<img src=\"RL_25.png\" width=\"300\" style=\"left:1px\">  \n",
    "\n",
    "\n",
    "### Finding the Optimal Policy\n",
    "\n",
    "<img src=\"RL_26.png\" width=\"400\" style=\"left:1px\">  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Optimality Equation\n",
    "What is the optimal value of being in state S ?. We consider each of the actions we might take, and an action will take us to one of the chance nodes. We look at the action value for each action.\n",
    "\n",
    "Instead of taking average like in Bellman expectation equation, we take the maximum of q*(s, a), and that tells us how good is it to be in that state S.\n",
    "\n",
    "**Bellman optimality equation for V* **\n",
    "\n",
    "<img src=\"RL_27.png\" width=\"300\" style=\"left:1px\">  \n",
    "\n",
    "**Bellman optimality equation for Q* **\n",
    "\n",
    "<img src=\"RL_28.png\" width=\"300\" style=\"left:1px\">  \n",
    "\n",
    "**Stitching Bellman optimality equation for V*(s),**\n",
    "\n",
    "<img src=\"RL_29.png\" width=\"300\" style=\"left:1px\">  \n",
    "\n",
    "**Stitching Bellman optimality equation for Q*(s, a),**\n",
    "\n",
    "<img src=\"RL_30.png\" width=\"400\" >  \n",
    "\n",
    "\n",
    "\n",
    "## Solving The Bellman Optimality Equation\n",
    "\n",
    "Bellman optimality equation is a non-linear equation,there is no closed form solution in general, but there are many iterative solution methods that we can apply, \n",
    "     1. Value iteration.\n",
    "     2. Policy iteration.\n",
    "     3. Q-leaning\n",
    "     4. Sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
