{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Dynamic Programming\n",
    "  Dynamic Programming or (DP) is a **method for solving complex problems** by breaking them down into subproblems, solve the subproblems, and combine solutions to the subproblems to solve the overall problem.\n",
    "  \n",
    "  It is the general solution for the problems which has the following two properties:\n",
    "      \n",
    "      1. \"Optimal Structure\" where the principle of Optimality applies.\n",
    "                It tells us that we can solve some overall problem by breaking it down into two or more pieces, solve for            each of the pieces, and the optimal solution to the pieces tells us how to get the optimal solution to the overall          problem.\n",
    "      \n",
    "      2. ‚Äúoverlapping subproblems‚Äù, which means that the subproblems which occur once, will occur again and again.\n",
    "                 We broke down the problem of getting to the wall into first getting to the midpoint, and then getting to \n",
    "         the wall. That will help us solve the subproblem of how to get from another point to the wall. Solutions can be              cached and reused.\n",
    "    \n",
    "    **Markov Decision Processes satisfy both mentioned properties**\n",
    "\n",
    "There are two main ideas we tackle in a given MDP. If someone tells us the MDP, where M = (S, A, P, R, ùõæ), and a policy ùúã or an MRP where M = (S, P, R, ùõæ), we can do prediction, i.e. evaluate the given policy to get the value function on that policy.\n",
    "\n",
    "The other main idea is, we are given an MDP, M = (S, A, P, R, ùõæ) and we are asked for the optimal value function and optimal policy. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "In policy evaluation, we‚Äôre given an MDP and a policy ùúã. We try to evaluate the given policy. We can evaluate the given policy through iterative application of Bellman expectation equation, and later use the Bellman optimality equation to do control.\n",
    "\n",
    "\n",
    "## Policy Improvement\n",
    "Our reason for computing the value function for a policy is to help find better policies. Suppose we have determined the value function Vùúã for an arbitrary deterministic policy ùúã. For some state s we would like to know whether or not we should change the policy to deterministically choose an action a ‚â† ùúã(s).\n",
    "\n",
    "It would be better to consider the action at the specific state.\n",
    "\n",
    "So formally, given a policy ùúã, we‚Äôre going to evaluate it, i.e. we‚Äôre going to figure out a value function for that policy and then we improve the policy by acting greedily with respect to Vùúã\n",
    "\n",
    "<img src=\"images/RL_31.png\" width=\"400\" style=\"left:1px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "\n",
    "**Any optimal policy can be subdivided into two components; an optimal first action, followed by an optimal policy from successor state s‚Ä≤.**\n",
    "\n",
    "If our first action is optimal and after that we follow an optimal policy from wherever we end up, then we can say that the overall behavior is optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
